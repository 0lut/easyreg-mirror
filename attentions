In current mermaid version, the network used is unet , with none normalization, t the last layer is group normalized,the weighted linear softmax is used.
we found the small omt or range_loss is useful at the begining of the train
we may not consider the loss of the regularization term at the beginning of the train,
when a ldddmm network is well initialized, then fixed the momentum generation network to fine tune the  smoother network with very small regularization
then, train momentum network and smoother network together with small regularization

3.2 update
the current setting is simpleunet, with last layer group normalzation,
we use use softmax, disable compute weights from preweights, since the map is smooth
the omt loss is set as 0.01.
if you want to change the setttings of the smoothers, make sure there are two smoother setttings to be changed, one in deep smoother
one in forward method


3.3 update
maybe momentum is the better choice for the network input


3.9
need to double check the spacing amplified in resampling image

3.17
any version before cur_settings_adpt_lddmm_new use the 'standardize_divide_input_images', with value 0.5
while current we use is 2

3.21
to do: remove the neumam boundary condition in forward method, so see if it can accelerate the algrithom : no
to do: using spacing transform trick to see if can accerate the forward

3.22 currently all the boundary settings are follow the old one, we will turn into new one one the acceraltion finished
